{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@title Setup\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "# 1. Install MLC LLM Nightly (Force Reinstall to ensure clean state)\n",
        "print(\"üì¶ Installing MLC LLM and dependencies...\")\n",
        "!python -m pip install --pre -U -f https://mlc.ai/wheels mlc-llm-nightly-cu128 mlc-ai-nightly-cu128\n",
        "!apt-get install -y git-lfs\n"
      ],
      "metadata": {
        "id": "pBK2wCzN7tI8",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://www.github.com/emscripten-core/emsdk.git\n",
        "!cd /content/emsdk && ./emsdk install tot\n",
        "!cd /content/emsdk && ./emsdk activate tot"
      ],
      "metadata": {
        "id": "J_Su6LiJhz0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# 1. Install Rust (Standard script)\n",
        "# We use -y to say \"yes\" to prompts automatically\n",
        "!curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y\n",
        "\n",
        "# 2. Add Rust to the system PATH for this session\n",
        "# (Colab doesn't automatically load the path after install)\n",
        "os.environ['PATH'] += \":/root/.cargo/bin\"\n",
        "!rustup target add wasm32-unknown-emscripten\n",
        "\n",
        "# 2. Verify it is installed\n",
        "print(\"‚úÖ Target installed. Verifying...\")\n",
        "!rustup target list --installed"
      ],
      "metadata": {
        "id": "RXf5_Z1WjC4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "set -e\n",
        "\n",
        "# 1. Install Rust if missing\n",
        "if [ ! -f \"$HOME/.cargo/env\" ]; then\n",
        "    echo \"ü¶Ä Installing Rust...\"\n",
        "    curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y\n",
        "    source \"$HOME/.cargo/env\"\n",
        "    rustup target add wasm32-unknown-emscripten\n",
        "else\n",
        "    source \"$HOME/.cargo/env\"\n",
        "fi\n",
        "\n",
        "# 2. Install Emscripten if missing\n",
        "if [ ! -d \"/content/emsdk\" ]; then\n",
        "    echo \"üîß Installing Emscripten...\"\n",
        "    git clone https://github.com/emscripten-core/emsdk.git /content/emsdk\n",
        "    cd /content/emsdk\n",
        "    ./emsdk install latest\n",
        "    ./emsdk activate latest\n",
        "fi\n",
        "source /content/emsdk/emsdk_env.sh\n",
        "\n",
        "# 3. Clone MLC LLM if it doesn't exist\n",
        "if [ ! -d \"/content/mlc-llm\" ]; then\n",
        "    echo \"üìÇ Cloning MLC LLM...\"\n",
        "    git clone --recursive https://github.com/mlc-ai/mlc-llm.git /content/mlc-llm\n",
        "fi\n",
        "\n",
        "# 4. Build the Web Runtime\n",
        "cd /content/mlc-llm\n",
        "\n",
        "# Pre-requisite: Prepare Emscripten dependencies\n",
        "./web/prep_emcc_deps.sh\n",
        "\n",
        "# Create build directory\n",
        "mkdir -p build/wasm\n",
        "cd build/wasm\n",
        "\n",
        "# Configure with emcmake\n",
        "emcmake cmake ../.. \\\n",
        "    -DCMAKE_BUILD_TYPE=Release \\\n",
        "    -DUSE_WEBGPU=ON \\\n",
        "    -DUSE_WASM=ON \\\n",
        "    -DCMAKE_CXX_FLAGS=\"-O3\"\n",
        "\n",
        "# Compile\n",
        "make -j$(nproc) && make install\n",
        "echo \"‚úÖ Build Complete!\""
      ],
      "metadata": {
        "id": "6TiL9Plxhzxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K621e5gEjzgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Command\n",
        "\n",
        "python -m mlc_llm convert_weight /content/dist/models/vicuna-7b-v1.5/ --quantization q4f32_1 -o /content/vc7b\n"
      ],
      "metadata": {
        "id": "8pyfMJQa7t49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title gen config\n",
        "\n",
        "import json\n",
        "import os\n",
        "\n",
        "# 1. Settings (Must match your previous steps)\n",
        "# We point to where your SHARDS are located (/content/vc7b)\n",
        "OUTPUT_DIR = \"/content/vc7b\"\n",
        "QUANTIZATION = \"q4f32_1\"\n",
        "\n",
        "# 2. Define the Standard Vicuna 1.5 Configuration\n",
        "# This replicates exactly what the tool *should* have generated.\n",
        "config_data = {\n",
        "    \"model_type\": \"llama\",\n",
        "    \"quantization\": QUANTIZATION,\n",
        "    \"model_config\": {\n",
        "        \"hidden_size\": 4096,\n",
        "        \"intermediate_size\": 11008,\n",
        "        \"num_attention_heads\": 32,\n",
        "        \"num_hidden_layers\": 32,\n",
        "        \"rms_norm_eps\": 1e-05,\n",
        "        \"vocab_size\": 32000,\n",
        "        \"position_embedding_base\": 10000.0,\n",
        "        \"context_window_size\": 4096,\n",
        "        \"prefill_chunk_size\": 4096,\n",
        "        \"tensor_parallel_shards\": 1,\n",
        "        \"head_dim\": 128,\n",
        "        \"dtype\": \"float32\"\n",
        "    },\n",
        "    \"vocab_size\": 32000,\n",
        "    \"context_window_size\": 4096,\n",
        "    \"sliding_window_size\": -1,\n",
        "    \"prefill_chunk_size\": 4096,\n",
        "    \"attention_sink_size\": -1,\n",
        "    \"tensor_parallel_shards\": 1,\n",
        "    # This is the part the CLI failed to generate:\n",
        "    \"conv_template\": {\n",
        "        \"name\": \"vicuna_v1.1\",\n",
        "        \"system_template\": \"{system_message}\",\n",
        "        \"system_message\": \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\",\n",
        "        \"roles\": {\n",
        "            \"user\": \"USER\",\n",
        "            \"assistant\": \"ASSISTANT\"\n",
        "        },\n",
        "        \"role_msg_sep\": \" \",\n",
        "        \"role_empty_sep\": \" \",\n",
        "        \"seps\": [\n",
        "            \" \",\n",
        "            \"</s>\"\n",
        "        ],\n",
        "        \"stop_str\": [\n",
        "            \"</s>\"\n",
        "        ],\n",
        "        \"stop_token_ids\": [\n",
        "            2\n",
        "        ],\n",
        "        \"add_bos\": True\n",
        "    }\n",
        "}\n",
        "\n",
        "# 3. Write the file\n",
        "output_file = os.path.join(OUTPUT_DIR, \"mlc-chat-config.json\")\n",
        "\n",
        "# Ensure directory exists (it should, since you have shards there)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "with open(output_file, \"w\") as f:\n",
        "    json.dump(config_data, f, indent=2)\n",
        "\n",
        "print(f\"‚úÖ Successfully created config file at: {output_file}\")\n",
        "print(\"   You can now proceed to upload the '/content/vc7b' folder to Hugging Face.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "0uUV62bZ7TPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --recursive https://github.com/mlc-ai/mlc-llm.git"
      ],
      "metadata": {
        "id": "xv4YlNnQeE1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section\n",
        "\n",
        "cd mlc-llm\n",
        "\n",
        "ln -sf mlc_wasm_runtime.bc wasm_runtime.bc\n",
        "\n",
        " ./web/prep_emcc_deps.sh\n",
        "\n",
        "cd web/dist/wasm\n",
        "\n",
        "ln -sf mlc_wasm_runtime.bc wasm_runtime.bc\n",
        "\n",
        "export TVM_LIBRARY_PATH=$PWD/web/dist/wasm\n",
        "\n",
        "export TVM_HOME=$PWD/3rdparty/tvm\n",
        "\n"
      ],
      "metadata": {
        "id": "cSfVa9fRdc6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title get config\n",
        "!wget https://huggingface.co/ford442/vicuna-7b-q4f32-webllm/resolve/main/mlc-chat-config.json"
      ],
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "-D_KTyG48HDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile wasm command:"
      ],
      "metadata": {
        "id": "kU675eauVtiR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "python -m mlc_llm compile /content/mlc-chat-config.json --device webgpu -o /content/vicuna_model.wasm"
      ],
      "metadata": {
        "id": "hMr-UJp7VCe8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uYMaG1AJVCMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from huggingface_hub import login, HfApi\n",
        "\n",
        "# 1. PASTE NEW \"WRITE\" TOKEN HERE\n",
        "HF_TOKEN = \"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\"\n",
        "\n",
        "# 2. VERIFY THIS IS YOUR USERNAME\n",
        "# If your username is NOT 'ford442', change it here.\n",
        "USERNAME = \"ford442\"\n",
        "MODEL_NAME = \"vicuna-7b-q4f32-web\"\n",
        "REPO_ID = f\"{USERNAME}/{MODEL_NAME}\"\n",
        "\n",
        "FOLDER_PATH = \"/content/vc7b\"\n",
        "\n",
        "# --- Upload Process ---\n",
        "print(f\"üîë Logging in...\")\n",
        "try:\n",
        "    login(token=HF_TOKEN)\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Login failed: {e}\")\n",
        "\n",
        "print(f\"\\nüöÄ Creating repo: {REPO_ID}\")\n",
        "api = HfApi()\n",
        "\n",
        "try:\n",
        "    # Create repo (if it fails here, the token or username is still wrong)\n",
        "    api.create_repo(repo_id=REPO_ID, repo_type=\"model\", exist_ok=True)\n",
        "\n",
        "    # Upload\n",
        "    print(f\"üì§ Uploading files from {FOLDER_PATH}...\")\n",
        "    api.upload_folder(\n",
        "        folder_path=FOLDER_PATH,\n",
        "        repo_id=REPO_ID,\n",
        "        repo_type=\"model\"\n",
        "    )\n",
        "    print(f\"\\n‚úÖ Success! Your model is live: https://huggingface.co/{REPO_ID}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Error: {e}\")\n",
        "    print(\"Double check that:\")\n",
        "    print(\"1. Your token is a 'WRITE' token.\")\n",
        "    print(f\"2. You are actually the user '{USERNAME}' on Hugging Face.\")"
      ],
      "metadata": {
        "id": "Dbap-Wvy9LeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "from huggingface_hub import login, HfApi\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# PASTE YOUR WRITE TOKEN BELOW (Keep the quotes!)\n",
        "# ---------------------------------------------------------\n",
        "HF_TOKEN = \"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\"\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Settings\n",
        "# ---------------------------------------------------------\n",
        "REPO_ID = \"ford442/vicuna-7b-webllm-q4f32\"\n",
        "FOLDER_PATH = \"/content/vc7b\"\n",
        "\n",
        "# 1. Login directly using the token string\n",
        "print(f\"üîë Logging in with provided token...\")\n",
        "login(token=HF_TOKEN)\n",
        "\n",
        "# 2. Upload\n",
        "print(f\"\\nüöÄ Uploading {FOLDER_PATH} to {REPO_ID}...\")\n",
        "api = HfApi()\n",
        "\n",
        "# Create the repo if it doesn't exist\n",
        "api.create_repo(repo_id=REPO_ID, repo_type=\"model\", exist_ok=True)\n",
        "\n",
        "# Upload folder\n",
        "api.upload_folder(\n",
        "    folder_path=FOLDER_PATH,\n",
        "    repo_id=REPO_ID,\n",
        "    repo_type=\"model\"\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Done! Your model is live at: https://huggingface.co/{REPO_ID}\")"
      ],
      "metadata": {
        "id": "vadhhyvn8HBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nVqZOrJt8G-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "grD4NLLAwsiV"
      },
      "outputs": [],
      "source": [
        "%%shell\n",
        "set -e\n",
        "\n",
        "# 1. Install MLC LLM Python Package\n",
        "# We use the nightly build to match the runtime setup\n",
        "if ! python -c \"import mlc_llm\" &> /dev/null; then\n",
        "    echo \"üì¶ Installing MLC LLM...\"\n",
        "    python -m pip install --pre -U -f https://mlc.ai/wheels mlc-llm-nightly-cu128 mlc-ai-nightly-cu128\n",
        "    apt-get install -y git-lfs\n",
        "fi\n",
        "\n",
        "# 2. Setup Emscripten Environment (Required for WASM compilation)\n",
        "source /content/emsdk/emsdk_env.sh\n",
        "\n",
        "# 3. Download Model\n",
        "MODEL_ID=\"lmsys/vicuna-7b-v1.5\"\n",
        "MODEL_DIR=\"dist/models/vicuna-7b-v1.5\"\n",
        "\n",
        "echo \"‚¨áÔ∏è Downloading $MODEL_ID...\"\n",
        "git lfs install\n",
        "mkdir -p dist/models\n",
        "if [ ! -d \"$MODEL_DIR\" ]; then\n",
        "    git clone https://huggingface.co/$MODEL_ID $MODEL_DIR\n",
        "else\n",
        "    echo \"   Model directory exists, skipping clone.\"\n",
        "fi\n",
        "\n",
        "# 4. Define Output Paths\n",
        "QUANTIZATION=\"q4f32_1\"\n",
        "OUTPUT_NAME=\"vicuna-7b-v1.5-$QUANTIZATION-webllm\"\n",
        "OUTPUT_DIR=\"dist/$OUTPUT_NAME\"\n",
        "\n",
        "# 5. Convert Weights & Generate Config\n",
        "echo \"‚öôÔ∏è Converting weights to $QUANTIZATION...\"\n",
        "python -m mlc_llm convert_weight $MODEL_DIR/ \\\n",
        "    --quantization $QUANTIZATION \\\n",
        "    -o $OUTPUT_DIR\n",
        "\n",
        "echo \"üìù Generating config...\"\n",
        "python -m mlc_llm gen_config $MODEL_DIR/ \\\n",
        "    --quantization $QUANTIZATION \\\n",
        "    --conv-template vicuna_v1.1 \\\n",
        "    -o $OUTPUT_DIR\n",
        "\n",
        "# 6. Compile Model to WASM\n",
        "echo \"üî® Compiling model to WASM...\"\n",
        "python -m mlc_llm compile $OUTPUT_DIR/mlc-chat-config.json \\\n",
        "    --device webgpu \\\n",
        "    -o $OUTPUT_DIR/vicuna-7b-v1.5-$QUANTIZATION-webgpu.wasm\n",
        "\n",
        "echo \"‚úÖ Conversion and Compilation Complete!\"\n",
        "echo \"üìÇ Output contents of $OUTPUT_DIR:\"\n",
        "ls -lh $OUTPUT_DIR"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R-J9IS1Fwv55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mawWeqAkwv3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title WebLLM Model Converter\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "# 1. Install MLC LLM Nightly (Using cu128 to match Colab's latest environment)\n",
        "print(\"üì¶ Checking/Installing MLC LLM and dependencies...\")\n",
        "# We force reinstall to ensure we have the correct version matching the runtime\n",
        "!{sys.executable} -m pip install --pre --force-reinstall mlc-llm-nightly-cu128 mlc-ai-nightly-cu128 -f https://mlc.ai/wheels\n",
        "!apt-get install -y git-lfs\n",
        "\n",
        "# 2. Configuration\n",
        "MODEL_ID = \"lmsys/vicuna-7b-v1.5\"\n",
        "MODEL_NAME = MODEL_ID.split(\"/\")[-1]\n",
        "QUANTIZATION = \"q4f32_1\"\n",
        "\n",
        "# Create directories\n",
        "!mkdir -p dist/models\n",
        "\n",
        "# 3. Clone the Original Model\n",
        "print(f\"‚¨áÔ∏è Downloading {MODEL_ID} from HuggingFace...\")\n",
        "!git lfs install\n",
        "if not os.path.exists(f\"dist/models/{MODEL_NAME}\"):\n",
        "    !git clone https://huggingface.co/{MODEL_ID} dist/models/{MODEL_NAME}\n",
        "else:\n",
        "    print(\"   Model directory exists. Skipping clone (ensure it's complete).\")\n",
        "\n",
        "# 4. Conversion & Config Generation\n",
        "output_name = f\"{MODEL_NAME}-{QUANTIZATION}-MLC\"\n",
        "output_path = f\"dist/{output_name}\"\n",
        "\n",
        "print(f\"\\n‚öôÔ∏è Converting to {QUANTIZATION}...\")\n",
        "print(f\"   Input: dist/models/{MODEL_NAME}\")\n",
        "print(f\"   Output: {output_path}\")\n",
        "\n",
        "# Run conversion\n",
        "convert_cmd = f\"{sys.executable} -m mlc_llm convert_weight dist/models/{MODEL_NAME}/ --quantization {QUANTIZATION} -o {output_path}\"\n",
        "if os.system(convert_cmd) != 0:\n",
        "    raise Exception(\"Weight conversion failed! (Possible OOM - Try restarting runtime)\")\n",
        "\n",
        "# Run config generation\n",
        "print(\"\\nüìù Generating Configuration...\")\n",
        "config_cmd = f\"{sys.executable} -m mlc_llm gen_config dist/models/{MODEL_NAME}/ --quantization {QUANTIZATION} --conv-template vicuna_v1.1 -o {output_path}\"\n",
        "if os.system(config_cmd) != 0:\n",
        "    raise Exception(\"Config generation failed!\")\n",
        "\n",
        "print(f\"\\n‚úÖ Success! Model prepared at: {output_path}\")"
      ],
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "23rquTFewv05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title üöÄ Final Upload Script\n",
        "import os\n",
        "from huggingface_hub import login, HfApi\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1. PASTE YOUR NEW \"WRITE\" TOKEN HERE\n",
        "# ---------------------------------------------------------\n",
        "HF_TOKEN = \"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\"\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. CONFIGURATION\n",
        "# ---------------------------------------------------------\n",
        "# Based on your previous logs, your username is ford442\n",
        "USERNAME = \"ford442\"\n",
        "MODEL_NAME = \"vicuna-7b-q4f32-web\"\n",
        "REPO_ID = f\"{USERNAME}/{MODEL_NAME}\"\n",
        "FOLDER_PATH = \"/content/vc7b\"\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3. UPLOAD\n",
        "# ---------------------------------------------------------\n",
        "print(f\"üîë Logging in...\")\n",
        "login(token=HF_TOKEN)\n",
        "\n",
        "print(f\"\\nüöÄ Deploying model to: https://huggingface.co/{REPO_ID}\")\n",
        "api = HfApi()\n",
        "\n",
        "try:\n",
        "    # Create repo if it doesn't exist\n",
        "    api.create_repo(repo_id=REPO_ID, repo_type=\"model\", exist_ok=True)\n",
        "\n",
        "    # Upload all files\n",
        "    api.upload_folder(\n",
        "        folder_path=FOLDER_PATH,\n",
        "        repo_id=REPO_ID,\n",
        "        repo_type=\"model\"\n",
        "    )\n",
        "\n",
        "    print(f\"\\n‚úÖ SUCCESS! The model is live and ready for WebLLM.\")\n",
        "    print(f\"üîó Link: https://huggingface.co/{REPO_ID}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Error: {e}\")"
      ],
      "metadata": {
        "id": "KgFgc-sD9aJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f524ac8a"
      },
      "source": [
        "!pip install paramiko"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68589d69"
      },
      "source": [
        "import os\n",
        "import paramiko\n",
        "\n",
        "# --- Configuration ---\n",
        "LOCAL_DIR = \"dist/vicuna-7b-v1.5-q4f32_1-MLC\"  # Directory containing the shards\n",
        "REMOTE_HOST = \"1ink.us\"\n",
        "REMOTE_PORT = 22\n",
        "USERNAME = \"ford442\"\n",
        "PASSWORD = \"GoogleBez12!\"\n",
        "REMOTE_DIR = \"files/vicuna\"  # Destination folder on the server (relative to home)\n",
        "\n",
        "# --- Upload Script ---\n",
        "print(f\"üöÄ Connecting to {REMOTE_HOST}...\")\n",
        "transport = paramiko.Transport((REMOTE_HOST, REMOTE_PORT))\n",
        "transport.connect(username=USERNAME, password=PASSWORD)\n",
        "sftp = paramiko.SFTPClient.from_transport(transport)\n",
        "\n",
        "# helper to create remote dir recursively\n",
        "def mkdir_p(sftp, remote_directory):\n",
        "    if remote_directory == '/': return\n",
        "    dirs = remote_directory.split(\"/\")\n",
        "\n",
        "    current_dir = \"\"\n",
        "    # Handle absolute paths if provided, otherwise treat as relative\n",
        "    if remote_directory.startswith(\"/\"):\n",
        "        current_dir = \"/\"\n",
        "        if dirs and dirs[0] == \"\":\n",
        "            dirs.pop(0)\n",
        "\n",
        "    for dir_part in dirs:\n",
        "        if not dir_part: continue\n",
        "\n",
        "        if current_dir == \"\" or current_dir == \"/\":\n",
        "            current_dir += dir_part\n",
        "        else:\n",
        "            current_dir += f\"/{dir_part}\"\n",
        "\n",
        "        try:\n",
        "            sftp.stat(current_dir)\n",
        "        except IOError:\n",
        "            print(f\"üìÅ Creating remote directory: {current_dir}\")\n",
        "            try:\n",
        "                sftp.mkdir(current_dir)\n",
        "            except IOError as e:\n",
        "                print(f\"   ‚ö†Ô∏è Could not create {current_dir}: {e}\")\n",
        "                raise\n",
        "\n",
        "# Ensure remote directory exists\n",
        "try:\n",
        "    mkdir_p(sftp, REMOTE_DIR)\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error setting up directories: {e}\")\n",
        "    # We don't exit here immediately to allow debugging, but usually this is fatal\n",
        "    # sftp.close(); transport.close(); raise\n",
        "\n",
        "# Upload files\n",
        "if os.path.exists(LOCAL_DIR):\n",
        "    print(f\"üì§ Uploading files from {LOCAL_DIR} to {REMOTE_DIR}...\")\n",
        "    files = os.listdir(LOCAL_DIR)\n",
        "    for filename in files:\n",
        "        local_path = os.path.join(LOCAL_DIR, filename)\n",
        "        remote_path = f\"{REMOTE_DIR}/{filename}\"\n",
        "\n",
        "        if os.path.isfile(local_path):\n",
        "            print(f\"   - Uploading {filename}...\")\n",
        "            try:\n",
        "                sftp.put(local_path, remote_path)\n",
        "            except Exception as e:\n",
        "                print(f\"     ‚ùå Failed to upload {filename}: {e}\")\n",
        "\n",
        "    print(\"‚úÖ Upload process finished!\")\n",
        "else:\n",
        "    print(f\"‚ùå Local directory {LOCAL_DIR} not found. Did the previous step finish?\")\n",
        "\n",
        "sftp.close()\n",
        "transport.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e569a2ff"
      },
      "source": [
        "import shutil\n",
        "\n",
        "# 1. Create the alias locally\n",
        "source_cache = os.path.join(LOCAL_DIR, \"tensor-cache.json\")\n",
        "dest_cache = os.path.join(LOCAL_DIR, \"ndarray-cache.json\")\n",
        "\n",
        "if os.path.exists(source_cache):\n",
        "    shutil.copy(source_cache, dest_cache)\n",
        "    print(f\"‚úÖ Created ndarray-cache.json from tensor-cache.json\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è Could not find {source_cache}\")\n",
        "\n",
        "# 2. Upload only the new file\n",
        "print(f\"üöÄ Connecting to {REMOTE_HOST} to upload the alias...\")\n",
        "transport = paramiko.Transport((REMOTE_HOST, REMOTE_PORT))\n",
        "transport.connect(username=USERNAME, password=PASSWORD)\n",
        "sftp = paramiko.SFTPClient.from_transport(transport)\n",
        "\n",
        "local_file = dest_cache\n",
        "remote_file = f\"{REMOTE_DIR}/ndarray-cache.json\"\n",
        "\n",
        "if os.path.exists(local_file):\n",
        "    print(f\"üì§ Uploading ndarray-cache.json...\")\n",
        "    try:\n",
        "        #sftp.put(local_file, remote_file)\n",
        "        print(\"‚úÖ Upload success!\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Upload failed: {e}\")\n",
        "\n",
        "sftp.close()\n",
        "transport.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RJOht0C39aHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wudl5gIG9aEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BZ1p0EAk_Gjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "%%shell\n",
        "#sudo update-alternatives --set python3 /usr/bin/python3.13\n",
        "pip install paramiko"
      ],
      "metadata": {
        "id": "ktqC4xpt_GhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "local_path = \"/content/vc7b.zip\" #@param [\"sh4.1ijs\", \"sh5.1ijs\", \"g3007.wasm\", \"g3008.wasm\", \"g3009.wasm\", \"sh6.1ijs\", \"g3010.wasm\"] {allow-input: true}\n",
        "loc_file = \"vc7b.zip\" #@param [\"sh4.1ijs\", \"sh5.1ijs\", \"g3007.wasm\", \"g3008.wasm\", \"g3009.wasm\", \"sh6.1ijs\", \"g3010.wasm\"] {allow-input: true}\n",
        "dest_path = \"1ink.us/files/\" #@param [\"sh4.1ijs\", \"sh5.1ijs\", \"g3007.wasm\", \"g3008.wasm\", \"g3009.wasm\", \"sh6.1ijs\", \"g3010.wasm\"] {allow-input: true}\n",
        "import os\n",
        "import urllib\n",
        "import requests as reqs\n",
        "import re\n",
        "import paramiko\n",
        "host = \"1ink.us\"\n",
        "username  = \"ford442\"\n",
        "password  = \"GoogleBez12!\"\n",
        "port = 22\n",
        "file_name=loc_file\n",
        "transport = paramiko.Transport((host, port))\n",
        "destination_path=dest_path+file_name\n",
        "transport.connect(username = username, password = password)\n",
        "sftp = paramiko.SFTPClient.from_transport(transport)\n",
        "sftp.put(local_path, destination_path)\n",
        "sftp.close()\n",
        "transport.close()"
      ],
      "metadata": {
        "id": "nBeLE9hO_IEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51b6f031"
      },
      "source": [
        "# Task\n",
        "Compile the Vicuna-7b-v1.5 model into a WebAssembly (WASM) binary using MLC LLM, preparing it for deployment with WebLLM, and ensure the resulting WASM file is generated and its path is provided. This includes installing necessary build dependencies (git-lfs, cmake, Rust, Emscripten), cloning the MLC LLM repository, building the TVM Web Runtime, and then compiling the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69a3c6d4"
      },
      "source": [
        "## Install Build Dependencies\n",
        "\n",
        "### Subtask:\n",
        "Install essential build tools including git-lfs, cmake, Rust, and Emscripten. Clone the MLC LLM repository recursively and configure the Emscripten environment variables.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5292c2a4"
      },
      "source": [
        "**Reasoning**:\n",
        "The first instruction is to install MLC LLM nightly package and git-lfs. Cell `pBK2wCzN7tI8` in the provided notebook handles this installation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43d3042f"
      },
      "source": [
        "#@title Setup\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "# 1. Install MLC LLM Nightly (Force Reinstall to ensure clean state)\n",
        "print(\"üì¶ Installing MLC LLM and dependencies...\")\n",
        "!python -m pip install --pre -U -f https://mlc.ai/wheels mlc-llm-nightly-cu128 mlc-ai-nightly-cu128\n",
        "!apt-get install -y git-lfs"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}