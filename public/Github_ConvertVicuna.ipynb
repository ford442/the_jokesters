{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@title Setup\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "# 1. Install MLC LLM Nightly (Force Reinstall to ensure clean state)\n",
        "print(\"\ud83d\udce6 Installing MLC LLM and dependencies...\")\n",
        "!python -m pip install --pre -U -f https://mlc.ai/wheels mlc-llm-nightly-cu128 mlc-ai-nightly-cu128\n",
        "!apt-get install -y git-lfs\n"
      ],
      "metadata": {
        "id": "pBK2wCzN7tI8",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://www.github.com/emscripten-core/emsdk.git\n",
        "!cd /content/emsdk && ./emsdk install tot\n",
        "!cd /content/emsdk && ./emsdk activate tot"
      ],
      "metadata": {
        "id": "J_Su6LiJhz0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# 1. Install Rust (Standard script)\n",
        "# We use -y to say \"yes\" to prompts automatically\n",
        "!curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y\n",
        "\n",
        "# 2. Add Rust to the system PATH for this session\n",
        "# (Colab doesn't automatically load the path after install)\n",
        "os.environ['PATH'] += \":/root/.cargo/bin\"\n",
        "!rustup target add wasm32-unknown-emscripten\n",
        "\n",
        "# 2. Verify it is installed\n",
        "print(\"\u2705 Target installed. Verifying...\")\n",
        "!rustup target list --installed"
      ],
      "metadata": {
        "id": "RXf5_Z1WjC4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "set -e\n",
        "\n",
        "# 1. Install Rust if missing\n",
        "if [ ! -f \"$HOME/.cargo/env\" ]; then\n",
        "    echo \"\ud83e\udd80 Installing Rust...\"\n",
        "    curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y\n",
        "    source \"$HOME/.cargo/env\"\n",
        "    rustup target add wasm32-unknown-emscripten\n",
        "else\n",
        "    source \"$HOME/.cargo/env\"\n",
        "fi\n",
        "\n",
        "# 2. Install Emscripten if missing\n",
        "if [ ! -d \"/content/emsdk\" ]; then\n",
        "    echo \"\ud83d\udd27 Installing Emscripten...\"\n",
        "    git clone https://github.com/emscripten-core/emsdk.git /content/emsdk\n",
        "    cd /content/emsdk\n",
        "    ./emsdk install latest\n",
        "    ./emsdk activate latest\n",
        "fi\n",
        "source /content/emsdk/emsdk_env.sh\n",
        "\n",
        "# 3. Clone MLC LLM if it doesn't exist\n",
        "if [ ! -d \"/content/mlc-llm\" ]; then\n",
        "    echo \"\ud83d\udcc2 Cloning MLC LLM...\"\n",
        "    git clone --recursive https://github.com/mlc-ai/mlc-llm.git /content/mlc-llm\n",
        "fi\n",
        "\n",
        "# 4. Build the Web Runtime\n",
        "cd /content/mlc-llm\n",
        "\n",
        "# Pre-requisite: Prepare Emscripten dependencies\n",
        "./web/prep_emcc_deps.sh\n",
        "\n",
        "# Create build directory\n",
        "mkdir -p build/wasm\n",
        "cd build/wasm\n",
        "\n",
        "# Configure with emcmake\n",
        "emcmake cmake ../.. \\\n",
        "    -DCMAKE_BUILD_TYPE=Release \\\n",
        "    -DUSE_WEBGPU=ON \\\n",
        "    -DUSE_WASM=ON \\\n",
        "    -DCMAKE_CXX_FLAGS=\"-O3\"\n",
        "\n",
        "# Compile\n",
        "make -j$(nproc) && make install\n",
        "echo \"\u2705 Build Complete!\""
      ],
      "metadata": {
        "id": "6TiL9Plxhzxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K621e5gEjzgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Command\n",
        "\n",
        "python -m mlc_llm convert_weight /content/dist/models/vicuna-7b-v1.5/ --quantization q4f32_1 -o /content/vc7b\n"
      ],
      "metadata": {
        "id": "8pyfMJQa7t49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title gen config\n",
        "\n",
        "import json\n",
        "import os\n",
        "\n",
        "# 1. Settings (Must match your previous steps)\n",
        "# We point to where your SHARDS are located (/content/vc7b)\n",
        "OUTPUT_DIR = \"/content/vc7b\"\n",
        "QUANTIZATION = \"q4f32_1\"\n",
        "\n",
        "# 2. Define the Standard Vicuna 1.5 Configuration\n",
        "# This replicates exactly what the tool *should* have generated.\n",
        "config_data = {\n",
        "    \"model_type\": \"llama\",\n",
        "    \"quantization\": QUANTIZATION,\n",
        "    \"model_config\": {\n",
        "        \"hidden_size\": 4096,\n",
        "        \"intermediate_size\": 11008,\n",
        "        \"num_attention_heads\": 32,\n",
        "        \"num_hidden_layers\": 32,\n",
        "        \"rms_norm_eps\": 1e-05,\n",
        "        \"vocab_size\": 32000,\n",
        "        \"position_embedding_base\": 10000.0,\n",
        "        \"context_window_size\": 4096,\n",
        "        \"prefill_chunk_size\": 4096,\n",
        "        \"tensor_parallel_shards\": 1,\n",
        "        \"head_dim\": 128,\n",
        "        \"dtype\": \"float32\"\n",
        "    },\n",
        "    \"vocab_size\": 32000,\n",
        "    \"context_window_size\": 4096,\n",
        "    \"sliding_window_size\": -1,\n",
        "    \"prefill_chunk_size\": 4096,\n",
        "    \"attention_sink_size\": -1,\n",
        "    \"tensor_parallel_shards\": 1,\n",
        "    # This is the part the CLI failed to generate:\n",
        "    \"conv_template\": {\n",
        "        \"name\": \"vicuna_v1.1\",\n",
        "        \"system_template\": \"{system_message}\",\n",
        "        \"system_message\": \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\",\n",
        "        \"roles\": {\n",
        "            \"user\": \"USER\",\n",
        "            \"assistant\": \"ASSISTANT\"\n",
        "        },\n",
        "        \"role_msg_sep\": \" \",\n",
        "        \"role_empty_sep\": \" \",\n",
        "        \"seps\": [\n",
        "            \" \",\n",
        "            \"</s>\"\n",
        "        ],\n",
        "        \"stop_str\": [\n",
        "            \"</s>\"\n",
        "        ],\n",
        "        \"stop_token_ids\": [\n",
        "            2\n",
        "        ],\n",
        "        \"add_bos\": True\n",
        "    }\n",
        "}\n",
        "\n",
        "# 3. Write the file\n",
        "output_file = os.path.join(OUTPUT_DIR, \"mlc-chat-config.json\")\n",
        "\n",
        "# Ensure directory exists (it should, since you have shards there)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "with open(output_file, \"w\") as f:\n",
        "    json.dump(config_data, f, indent=2)\n",
        "\n",
        "print(f\"\u2705 Successfully created config file at: {output_file}\")\n",
        "print(\"   You can now proceed to upload the '/content/vc7b' folder to Hugging Face.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "0uUV62bZ7TPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --recursive https://github.com/mlc-ai/mlc-llm.git"
      ],
      "metadata": {
        "id": "xv4YlNnQeE1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section\n",
        "\n",
        "cd mlc-llm\n",
        "\n",
        "ln -sf mlc_wasm_runtime.bc wasm_runtime.bc\n",
        "\n",
        " ./web/prep_emcc_deps.sh\n",
        "\n",
        "cd web/dist/wasm\n",
        "\n",
        "ln -sf mlc_wasm_runtime.bc wasm_runtime.bc\n",
        "\n",
        "export TVM_LIBRARY_PATH=$PWD/web/dist/wasm\n",
        "\n",
        "export TVM_HOME=$PWD/3rdparty/tvm\n",
        "\n"
      ],
      "metadata": {
        "id": "cSfVa9fRdc6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title get config\n",
        "!wget https://huggingface.co/ford442/vicuna-7b-q4f32-webllm/resolve/main/mlc-chat-config.json"
      ],
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "-D_KTyG48HDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile wasm command:"
      ],
      "metadata": {
        "id": "kU675eauVtiR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "python -m mlc_llm compile /content/mlc-chat-config.json --device webgpu -o /content/vicuna_model.wasm"
      ],
      "metadata": {
        "id": "hMr-UJp7VCe8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uYMaG1AJVCMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from huggingface_hub import login, HfApi\n",
        "\n",
        "# 1. PASTE NEW \"WRITE\" TOKEN HERE\n",
        "HF_TOKEN = \"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\"\n",
        "\n",
        "# 2. VERIFY THIS IS YOUR USERNAME\n",
        "# If your username is NOT 'ford442', change it here.\n",
        "USERNAME = \"ford442\"\n",
        "MODEL_NAME = \"vicuna-7b-q4f32-web\"\n",
        "REPO_ID = f\"{USERNAME}/{MODEL_NAME}\"\n",
        "\n",
        "FOLDER_PATH = \"/content/vc7b\"\n",
        "\n",
        "# --- Upload Process ---\n",
        "print(f\"\ud83d\udd11 Logging in...\")\n",
        "try:\n",
        "    login(token=HF_TOKEN)\n",
        "except Exception as e:\n",
        "    print(f\"\u274c Login failed: {e}\")\n",
        "\n",
        "print(f\"\\n\ud83d\ude80 Creating repo: {REPO_ID}\")\n",
        "api = HfApi()\n",
        "\n",
        "try:\n",
        "    # Create repo (if it fails here, the token or username is still wrong)\n",
        "    api.create_repo(repo_id=REPO_ID, repo_type=\"model\", exist_ok=True)\n",
        "\n",
        "    # Upload\n",
        "    print(f\"\ud83d\udce4 Uploading files from {FOLDER_PATH}...\")\n",
        "    api.upload_folder(\n",
        "        folder_path=FOLDER_PATH,\n",
        "        repo_id=REPO_ID,\n",
        "        repo_type=\"model\"\n",
        "    )\n",
        "    print(f\"\\n\u2705 Success! Your model is live: https://huggingface.co/{REPO_ID}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n\u274c Error: {e}\")\n",
        "    print(\"Double check that:\")\n",
        "    print(\"1. Your token is a 'WRITE' token.\")\n",
        "    print(f\"2. You are actually the user '{USERNAME}' on Hugging Face.\")"
      ],
      "metadata": {
        "id": "Dbap-Wvy9LeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "from huggingface_hub import login, HfApi\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# PASTE YOUR WRITE TOKEN BELOW (Keep the quotes!)\n",
        "# ---------------------------------------------------------\n",
        "HF_TOKEN = \"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\"\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Settings\n",
        "# ---------------------------------------------------------\n",
        "REPO_ID = \"ford442/vicuna-7b-webllm-q4f32\"\n",
        "FOLDER_PATH = \"/content/vc7b\"\n",
        "\n",
        "# 1. Login directly using the token string\n",
        "print(f\"\ud83d\udd11 Logging in with provided token...\")\n",
        "login(token=HF_TOKEN)\n",
        "\n",
        "# 2. Upload\n",
        "print(f\"\\n\ud83d\ude80 Uploading {FOLDER_PATH} to {REPO_ID}...\")\n",
        "api = HfApi()\n",
        "\n",
        "# Create the repo if it doesn't exist\n",
        "api.create_repo(repo_id=REPO_ID, repo_type=\"model\", exist_ok=True)\n",
        "\n",
        "# Upload folder\n",
        "api.upload_folder(\n",
        "    folder_path=FOLDER_PATH,\n",
        "    repo_id=REPO_ID,\n",
        "    repo_type=\"model\"\n",
        ")\n",
        "\n",
        "print(f\"\u2705 Done! Your model is live at: https://huggingface.co/{REPO_ID}\")"
      ],
      "metadata": {
        "id": "vadhhyvn8HBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nVqZOrJt8G-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "grD4NLLAwsiV"
      },
      "outputs": [],
      "source": [
        "%%shell\n",
        "set -e\n",
        "\n",
        "# 1. Install MLC LLM Python Package\n",
        "# We use the nightly build to match the runtime setup\n",
        "if ! python -c \"import mlc_llm\" &> /dev/null; then\n",
        "    echo \"\ud83d\udce6 Installing MLC LLM...\"\n",
        "    python -m pip install --pre -U -f https://mlc.ai/wheels mlc-llm-nightly-cu128 mlc-ai-nightly-cu128\n",
        "    apt-get install -y git-lfs\n",
        "fi\n",
        "\n",
        "# 2. Setup Emscripten Environment (Required for WASM compilation)\n",
        "source /content/emsdk/emsdk_env.sh\n",
        "\n",
        "# 3. Download Model\n",
        "MODEL_ID=\"lmsys/vicuna-7b-v1.5\"\n",
        "MODEL_DIR=\"dist/models/vicuna-7b-v1.5\"\n",
        "\n",
        "echo \"\u2b07\ufe0f Downloading $MODEL_ID...\"\n",
        "git lfs install\n",
        "mkdir -p dist/models\n",
        "if [ ! -d \"$MODEL_DIR\" ]; then\n",
        "    git clone https://huggingface.co/$MODEL_ID $MODEL_DIR\n",
        "else\n",
        "    echo \"   Model directory exists, skipping clone.\"\n",
        "fi\n",
        "\n",
        "# 4. Define Output Paths\n",
        "QUANTIZATION=\"q4f32_1\"\n",
        "OUTPUT_NAME=\"vicuna-7b-v1.5-$QUANTIZATION-webllm\"\n",
        "OUTPUT_DIR=\"dist/$OUTPUT_NAME\"\n",
        "\n",
        "# 5. Convert Weights & Generate Config\n",
        "echo \"\u2699\ufe0f Converting weights to $QUANTIZATION...\"\n",
        "python -m mlc_llm convert_weight $MODEL_DIR/ \\\n",
        "    --quantization $QUANTIZATION \\\n",
        "    -o $OUTPUT_DIR\n",
        "\n",
        "echo \"\ud83d\udcdd Generating config...\"\n",
        "python -m mlc_llm gen_config $MODEL_DIR/ \\\n",
        "    --quantization $QUANTIZATION \\\n",
        "    --conv-template vicuna_v1.1 \\\n",
        "    -o $OUTPUT_DIR\n",
        "\n",
        "# 6. Compile Model to WASM\n",
        "echo \"\ud83d\udd28 Compiling model to WASM...\"\n",
        "python -m mlc_llm compile $OUTPUT_DIR/mlc-chat-config.json \\\n",
        "    --device webgpu \\\n",
        "    -o $OUTPUT_DIR/vicuna-7b-v1.5-$QUANTIZATION-webgpu.wasm\n",
        "\n",
        "echo \"\u2705 Conversion and Compilation Complete!\"\n",
        "echo \"\ud83d\udcc2 Output contents of $OUTPUT_DIR:\"\n",
        "ls -lh $OUTPUT_DIR"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R-J9IS1Fwv55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mawWeqAkwv3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title WebLLM Model Converter\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "# 1. Install MLC LLM Nightly (Using cu128 to match Colab's latest environment)\n",
        "print(\"\ud83d\udce6 Checking/Installing MLC LLM and dependencies...\")\n",
        "# We force reinstall to ensure we have the correct version matching the runtime\n",
        "!{sys.executable} -m pip install --pre --force-reinstall mlc-llm-nightly-cu128 mlc-ai-nightly-cu128 -f https://mlc.ai/wheels\n",
        "!apt-get install -y git-lfs\n",
        "\n",
        "# 2. Configuration\n",
        "MODEL_ID = \"lmsys/vicuna-7b-v1.5\"\n",
        "MODEL_NAME = MODEL_ID.split(\"/\")[-1]\n",
        "QUANTIZATION = \"q4f32_1\"\n",
        "\n",
        "# Create directories\n",
        "!mkdir -p dist/models\n",
        "\n",
        "# 3. Clone the Original Model\n",
        "print(f\"\u2b07\ufe0f Downloading {MODEL_ID} from HuggingFace...\")\n",
        "!git lfs install\n",
        "if not os.path.exists(f\"dist/models/{MODEL_NAME}\"):\n",
        "    !git clone https://huggingface.co/{MODEL_ID} dist/models/{MODEL_NAME}\n",
        "else:\n",
        "    print(\"   Model directory exists. Skipping clone (ensure it's complete).\")\n",
        "\n",
        "# 4. Conversion & Config Generation\n",
        "output_name = f\"{MODEL_NAME}-{QUANTIZATION}-MLC\"\n",
        "output_path = f\"dist/{output_name}\"\n",
        "\n",
        "print(f\"\\n\u2699\ufe0f Converting to {QUANTIZATION}...\")\n",
        "print(f\"   Input: dist/models/{MODEL_NAME}\")\n",
        "print(f\"   Output: {output_path}\")\n",
        "\n",
        "# Run conversion\n",
        "convert_cmd = f\"{sys.executable} -m mlc_llm convert_weight dist/models/{MODEL_NAME}/ --quantization {QUANTIZATION} -o {output_path}\"\n",
        "if os.system(convert_cmd) != 0:\n",
        "    raise Exception(\"Weight conversion failed! (Possible OOM - Try restarting runtime)\")\n",
        "\n",
        "# Run config generation\n",
        "print(\"\\n\ud83d\udcdd Generating Configuration...\")\n",
        "config_cmd = f\"{sys.executable} -m mlc_llm gen_config dist/models/{MODEL_NAME}/ --quantization {QUANTIZATION} --conv-template vicuna_v1.1 -o {output_path}\"\n",
        "if os.system(config_cmd) != 0:\n",
        "    raise Exception(\"Config generation failed!\")\n",
        "\n",
        "print(f\"\\n\u2705 Success! Model prepared at: {output_path}\")"
      ],
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "23rquTFewv05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title \ud83d\ude80 Final Upload Script\n",
        "import os\n",
        "from huggingface_hub import login, HfApi\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1. PASTE YOUR NEW \"WRITE\" TOKEN HERE\n",
        "# ---------------------------------------------------------\n",
        "HF_TOKEN = \"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\"\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. CONFIGURATION\n",
        "# ---------------------------------------------------------\n",
        "# Based on your previous logs, your username is ford442\n",
        "USERNAME = \"ford442\"\n",
        "MODEL_NAME = \"vicuna-7b-q4f32-web\"\n",
        "REPO_ID = f\"{USERNAME}/{MODEL_NAME}\"\n",
        "FOLDER_PATH = \"/content/vc7b\"\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3. UPLOAD\n",
        "# ---------------------------------------------------------\n",
        "print(f\"\ud83d\udd11 Logging in...\")\n",
        "login(token=HF_TOKEN)\n",
        "\n",
        "print(f\"\\n\ud83d\ude80 Deploying model to: https://huggingface.co/{REPO_ID}\")\n",
        "api = HfApi()\n",
        "\n",
        "try:\n",
        "    # Create repo if it doesn't exist\n",
        "    api.create_repo(repo_id=REPO_ID, repo_type=\"model\", exist_ok=True)\n",
        "\n",
        "    # Upload all files\n",
        "    api.upload_folder(\n",
        "        folder_path=FOLDER_PATH,\n",
        "        repo_id=REPO_ID,\n",
        "        repo_type=\"model\"\n",
        "    )\n",
        "\n",
        "    print(f\"\\n\u2705 SUCCESS! The model is live and ready for WebLLM.\")\n",
        "    print(f\"\ud83d\udd17 Link: https://huggingface.co/{REPO_ID}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n\u274c Error: {e}\")"
      ],
      "metadata": {
        "id": "KgFgc-sD9aJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f524ac8a"
      },
      "source": [
        "!pip install paramiko"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68589d69"
      },
      "source": [
        "import os\n",
        "import paramiko\n",
        "\n",
        "# --- Configuration ---\n",
        "LOCAL_DIR = \"dist/vicuna-7b-v1.5-q4f32_1-MLC\"  # Directory containing the shards\n",
        "REMOTE_HOST = \"1ink.us\"\n",
        "REMOTE_PORT = 22\n",
        "USERNAME = \"ford442\"\n",
        "PASSWORD = \"GoogleBez12!\"\n",
        "REMOTE_DIR = \"files/vicuna\"  # Destination folder on the server (relative to home)\n",
        "\n",
        "# --- Upload Script ---\n",
        "print(f\"\ud83d\ude80 Connecting to {REMOTE_HOST}...\")\n",
        "transport = paramiko.Transport((REMOTE_HOST, REMOTE_PORT))\n",
        "transport.connect(username=USERNAME, password=PASSWORD)\n",
        "sftp = paramiko.SFTPClient.from_transport(transport)\n",
        "\n",
        "# helper to create remote dir recursively\n",
        "def mkdir_p(sftp, remote_directory):\n",
        "    if remote_directory == '/': return\n",
        "    dirs = remote_directory.split(\"/\")\n",
        "\n",
        "    current_dir = \"\"\n",
        "    # Handle absolute paths if provided, otherwise treat as relative\n",
        "    if remote_directory.startswith(\"/\"):\n",
        "        current_dir = \"/\"\n",
        "        if dirs and dirs[0] == \"\":\n",
        "            dirs.pop(0)\n",
        "\n",
        "    for dir_part in dirs:\n",
        "        if not dir_part: continue\n",
        "\n",
        "        if current_dir == \"\" or current_dir == \"/\":\n",
        "            current_dir += dir_part\n",
        "        else:\n",
        "            current_dir += f\"/{dir_part}\"\n",
        "\n",
        "        try:\n",
        "            sftp.stat(current_dir)\n",
        "        except IOError:\n",
        "            print(f\"\ud83d\udcc1 Creating remote directory: {current_dir}\")\n",
        "            try:\n",
        "                sftp.mkdir(current_dir)\n",
        "            except IOError as e:\n",
        "                print(f\"   \u26a0\ufe0f Could not create {current_dir}: {e}\")\n",
        "                raise\n",
        "\n",
        "# Ensure remote directory exists\n",
        "try:\n",
        "    mkdir_p(sftp, REMOTE_DIR)\n",
        "except Exception as e:\n",
        "    print(f\"\u274c Error setting up directories: {e}\")\n",
        "    # We don't exit here immediately to allow debugging, but usually this is fatal\n",
        "    # sftp.close(); transport.close(); raise\n",
        "\n",
        "# Upload files\n",
        "if os.path.exists(LOCAL_DIR):\n",
        "    print(f\"\ud83d\udce4 Uploading files from {LOCAL_DIR} to {REMOTE_DIR}...\")\n",
        "    files = os.listdir(LOCAL_DIR)\n",
        "    for filename in files:\n",
        "        local_path = os.path.join(LOCAL_DIR, filename)\n",
        "        remote_path = f\"{REMOTE_DIR}/{filename}\"\n",
        "\n",
        "        if os.path.isfile(local_path):\n",
        "            print(f\"   - Uploading {filename}...\")\n",
        "            try:\n",
        "                sftp.put(local_path, remote_path)\n",
        "            except Exception as e:\n",
        "                print(f\"     \u274c Failed to upload {filename}: {e}\")\n",
        "\n",
        "    print(\"\u2705 Upload process finished!\")\n",
        "else:\n",
        "    print(f\"\u274c Local directory {LOCAL_DIR} not found. Did the previous step finish?\")\n",
        "\n",
        "sftp.close()\n",
        "transport.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e569a2ff"
      },
      "source": [
        "import shutil\n",
        "\n",
        "# 1. Create the alias locally\n",
        "source_cache = os.path.join(LOCAL_DIR, \"tensor-cache.json\")\n",
        "dest_cache = os.path.join(LOCAL_DIR, \"ndarray-cache.json\")\n",
        "\n",
        "if os.path.exists(source_cache):\n",
        "    shutil.copy(source_cache, dest_cache)\n",
        "    print(f\"\u2705 Created ndarray-cache.json from tensor-cache.json\")\n",
        "else:\n",
        "    print(f\"\u26a0\ufe0f Could not find {source_cache}\")\n",
        "\n",
        "# 2. Upload only the new file\n",
        "print(f\"\ud83d\ude80 Connecting to {REMOTE_HOST} to upload the alias...\")\n",
        "transport = paramiko.Transport((REMOTE_HOST, REMOTE_PORT))\n",
        "transport.connect(username=USERNAME, password=PASSWORD)\n",
        "sftp = paramiko.SFTPClient.from_transport(transport)\n",
        "\n",
        "local_file = dest_cache\n",
        "remote_file = f\"{REMOTE_DIR}/ndarray-cache.json\"\n",
        "\n",
        "if os.path.exists(local_file):\n",
        "    print(f\"\ud83d\udce4 Uploading ndarray-cache.json...\")\n",
        "    try:\n",
        "        #sftp.put(local_file, remote_file)\n",
        "        print(\"\u2705 Upload success!\")\n",
        "    except Exception as e:\n",
        "        print(f\"\u274c Upload failed: {e}\")\n",
        "\n",
        "sftp.close()\n",
        "transport.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RJOht0C39aHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wudl5gIG9aEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BZ1p0EAk_Gjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "%%shell\n",
        "#sudo update-alternatives --set python3 /usr/bin/python3.13\n",
        "pip install paramiko"
      ],
      "metadata": {
        "id": "ktqC4xpt_GhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "local_path = \"/content/vc7b.zip\" #@param [\"sh4.1ijs\", \"sh5.1ijs\", \"g3007.wasm\", \"g3008.wasm\", \"g3009.wasm\", \"sh6.1ijs\", \"g3010.wasm\"] {allow-input: true}\n",
        "loc_file = \"vc7b.zip\" #@param [\"sh4.1ijs\", \"sh5.1ijs\", \"g3007.wasm\", \"g3008.wasm\", \"g3009.wasm\", \"sh6.1ijs\", \"g3010.wasm\"] {allow-input: true}\n",
        "dest_path = \"1ink.us/files/\" #@param [\"sh4.1ijs\", \"sh5.1ijs\", \"g3007.wasm\", \"g3008.wasm\", \"g3009.wasm\", \"sh6.1ijs\", \"g3010.wasm\"] {allow-input: true}\n",
        "import os\n",
        "import urllib\n",
        "import requests as reqs\n",
        "import re\n",
        "import paramiko\n",
        "host = \"1ink.us\"\n",
        "username  = \"ford442\"\n",
        "password  = \"GoogleBez12!\"\n",
        "port = 22\n",
        "file_name=loc_file\n",
        "transport = paramiko.Transport((host, port))\n",
        "destination_path=dest_path+file_name\n",
        "transport.connect(username = username, password = password)\n",
        "sftp = paramiko.SFTPClient.from_transport(transport)\n",
        "sftp.put(local_path, destination_path)\n",
        "sftp.close()\n",
        "transport.close()"
      ],
      "metadata": {
        "id": "nBeLE9hO_IEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51b6f031"
      },
      "source": [
        "# Task\n",
        "Compile the Vicuna-7b-v1.5 model into a WebAssembly (WASM) binary using MLC LLM, preparing it for deployment with WebLLM, and ensure the resulting WASM file is generated and its path is provided. This includes installing necessary build dependencies (git-lfs, cmake, Rust, Emscripten), cloning the MLC LLM repository, building the TVM Web Runtime, and then compiling the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69a3c6d4"
      },
      "source": [
        "## Install Build Dependencies\n",
        "\n",
        "### Subtask:\n",
        "Install essential build tools including git-lfs, cmake, Rust, and Emscripten. Clone the MLC LLM repository recursively and configure the Emscripten environment variables.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5292c2a4"
      },
      "source": [
        "**Reasoning**:\n",
        "The first instruction is to install MLC LLM nightly package and git-lfs. Cell `pBK2wCzN7tI8` in the provided notebook handles this installation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43d3042f"
      },
      "source": [
        "#@title Setup\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "# 1. Install MLC LLM Nightly (Force Reinstall to ensure clean state)\n",
        "print(\"\ud83d\udce6 Installing MLC LLM and dependencies...\")\n",
        "!python -m pip install --pre -U -f https://mlc.ai/wheels mlc-llm-nightly-cu128 mlc-ai-nightly-cu128\n",
        "!apt-get install -y git-lfs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "test_existing_model_header"
      },
      "source": [
        "# Testing Existing HuggingFace Model\n",
        "\n",
        "This section demonstrates how to test the pre-converted `ford442/vicuna-7b-q4f32-webllm` model directly without reconversion."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "download_hf_model",
        "cellView": "form"
      },
      "source": [
        "#@title \ud83d\udce5 Download Pre-converted Model from HuggingFace\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "# Model configuration\n",
        "HF_MODEL_ID = \"ford442/vicuna-7b-q4f32-webllm\"\n",
        "MODEL_DIR = \"/content/ford442-vicuna-7b-q4f32\"\n",
        "\n",
        "print(f\"\ud83d\udce6 Downloading model: {HF_MODEL_ID}\")\n",
        "print(f\"\ud83d\udcc2 Destination: {MODEL_DIR}\\n\")\n",
        "\n",
        "# Install huggingface_hub if not present\n",
        "try:\n",
        "    from huggingface_hub import snapshot_download\n",
        "except ImportError:\n",
        "    print(\"Installing huggingface_hub...\")\n",
        "    !pip install -q huggingface_hub\n",
        "    from huggingface_hub import snapshot_download\n",
        "\n",
        "# Download the model\n",
        "try:\n",
        "    model_path = snapshot_download(\n",
        "        repo_id=HF_MODEL_ID,\n",
        "        local_dir=MODEL_DIR,\n",
        "        local_dir_use_symlinks=False\n",
        "    )\n",
        "    print(f\"\\n\u2705 Model downloaded successfully to: {model_path}\")\n",
        "    \n",
        "    # List downloaded files\n",
        "    print(\"\\n\ud83d\udccb Downloaded files:\")\n",
        "    !ls -lh {MODEL_DIR}\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\u274c Error downloading model: {e}\")\n",
        "    raise"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cli_chat_test",
        "cellView": "form"
      },
      "source": [
        "#@title \ud83d\udcac CLI Chat Test\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Ensure MLC LLM is installed\n",
        "try:\n",
        "    import mlc_llm\n",
        "except ImportError:\n",
        "    print(\"Installing MLC LLM...\")\n",
        "    !python -m pip install --pre -U -f https://mlc.ai/wheels mlc-llm-nightly-cu128 mlc-ai-nightly-cu128\n",
        "    import mlc_llm\n",
        "\n",
        "MODEL_DIR = \"/content/ford442-vicuna-7b-q4f32\"\n",
        "\n",
        "print(\"\ud83e\udd16 Starting CLI chat with Vicuna-7B...\")\n",
        "print(\"   Model path:\", MODEL_DIR)\n",
        "print(\"   Type your message and press Enter.\\n\")\n",
        "\n",
        "# Simple CLI chat test\n",
        "from mlc_llm import MLCEngine\n",
        "\n",
        "# Create model path\n",
        "model_path = MODEL_DIR\n",
        "\n",
        "# Initialize engine\n",
        "print(\"\u2699\ufe0f Initializing MLC Engine...\\n\")\n",
        "try:\n",
        "    engine = MLCEngine(model=model_path)\n",
        "    \n",
        "    # Test prompt\n",
        "    test_message = \"Hello! Tell me a short joke about AI.\"\n",
        "    print(f\"\ud83d\udc64 User: {test_message}\\n\")\n",
        "    print(\"\ud83e\udd16 Assistant: \", end=\"\", flush=True)\n",
        "    \n",
        "    # Generate response\n",
        "    response = \"\"\n",
        "    for chunk in engine.chat.completions.create(\n",
        "        messages=[{\"role\": \"user\", \"content\": test_message}],\n",
        "        model=model_path,\n",
        "        stream=True,\n",
        "        max_tokens=256\n",
        "    ):\n",
        "        if chunk.choices:\n",
        "            delta = chunk.choices[0].delta.content\n",
        "            if delta:\n",
        "                print(delta, end=\"\", flush=True)\n",
        "                response += delta\n",
        "    \n",
        "    print(\"\\n\\n\u2705 CLI chat test completed successfully!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\\n\u274c Error during CLI chat: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "web_chat_demo",
        "cellView": "form"
      },
      "source": [
        "#@title \ud83c\udf10 Web Chat Demo (WebLLM)\n",
        "import os\n",
        "from IPython.display import HTML, display\n",
        "import json\n",
        "\n",
        "MODEL_DIR = \"/content/ford442-vicuna-7b-q4f32\"\n",
        "\n",
        "print(\"\ud83c\udf10 Setting up WebLLM demo...\\n\")\n",
        "\n",
        "# Create a simple HTML demo that uses WebLLM\n",
        "html_content = '''\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "    <meta charset=\"utf-8\">\n",
        "    <title>WebLLM Chat Demo</title>\n",
        "    <style>\n",
        "        body { \n",
        "            font-family: Arial, sans-serif; \n",
        "            max-width: 800px; \n",
        "            margin: 20px auto; \n",
        "            padding: 20px;\n",
        "            background: #f5f5f5;\n",
        "        }\n",
        "        #chat-box {\n",
        "            background: white;\n",
        "            border: 1px solid #ddd;\n",
        "            border-radius: 8px;\n",
        "            padding: 20px;\n",
        "            height: 400px;\n",
        "            overflow-y: auto;\n",
        "            margin-bottom: 20px;\n",
        "        }\n",
        "        .message {\n",
        "            margin: 10px 0;\n",
        "            padding: 10px;\n",
        "            border-radius: 5px;\n",
        "        }\n",
        "        .user-message { background: #e3f2fd; text-align: right; }\n",
        "        .assistant-message { background: #f1f8e9; }\n",
        "        .system-message { background: #fff3e0; font-style: italic; }\n",
        "        #input-area {\n",
        "            display: flex;\n",
        "            gap: 10px;\n",
        "        }\n",
        "        #user-input {\n",
        "            flex: 1;\n",
        "            padding: 10px;\n",
        "            border: 1px solid #ddd;\n",
        "            border-radius: 5px;\n",
        "            font-size: 14px;\n",
        "        }\n",
        "        button {\n",
        "            padding: 10px 20px;\n",
        "            background: #2196f3;\n",
        "            color: white;\n",
        "            border: none;\n",
        "            border-radius: 5px;\n",
        "            cursor: pointer;\n",
        "            font-size: 14px;\n",
        "        }\n",
        "        button:hover { background: #1976d2; }\n",
        "        button:disabled { background: #ccc; cursor: not-allowed; }\n",
        "        #status { \n",
        "            margin: 10px 0; \n",
        "            padding: 10px;\n",
        "            background: #fff;\n",
        "            border-radius: 5px;\n",
        "            border: 1px solid #ddd;\n",
        "        }\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <h1>\ud83e\udd16 WebLLM Chat Demo - Vicuna 7B</h1>\n",
        "    <div id=\"status\">\u23f3 Initializing WebGPU and loading model...</div>\n",
        "    <div id=\"chat-box\"></div>\n",
        "    <div id=\"input-area\">\n",
        "        <input type=\"text\" id=\"user-input\" placeholder=\"Type your message...\" disabled>\n",
        "        <button id=\"send-btn\" onclick=\"sendMessage()\" disabled>Send</button>\n",
        "    </div>\n",
        "\n",
        "    <script type=\"module\">\n",
        "        import * as webllm from \"https://esm.run/@mlc-ai/web-llm\";\n",
        "\n",
        "        let engine = null;\n",
        "        const chatBox = document.getElementById('chat-box');\n",
        "        const statusDiv = document.getElementById('status');\n",
        "        const userInput = document.getElementById('user-input');\n",
        "        const sendBtn = document.getElementById('send-btn');\n",
        "\n",
        "        // Configure the model\n",
        "        const modelConfig = {\n",
        "            model_id: \"ford442/vicuna-7b-q4f32-webllm\",\n",
        "            model: \"https://huggingface.co/ford442/vicuna-7b-q4f32-webllm/resolve/main/\",\n",
        "            model_lib: \"https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Llama-2-7b-chat-hf-q4f32_1-ctx4k_cs1k-webgpu.wasm\",\n",
        "            vram_required_MB: 4096,\n",
        "            low_resource_required: false,\n",
        "        };\n",
        "\n",
        "        function addMessage(role, content) {\n",
        "            const div = document.createElement('div');\n",
        "            div.className = `message ${role}-message`;\n",
        "            div.textContent = content;\n",
        "            chatBox.appendChild(div);\n",
        "            chatBox.scrollTop = chatBox.scrollHeight;\n",
        "        }\n",
        "\n",
        "        function updateStatus(message, isError = false) {\n",
        "            statusDiv.textContent = message;\n",
        "            statusDiv.style.background = isError ? '#ffebee' : '#fff';\n",
        "        }\n",
        "\n",
        "        async function initEngine() {\n",
        "            try {\n",
        "                updateStatus('\ud83d\udd27 Checking WebGPU support...');\n",
        "                \n",
        "                if (!navigator.gpu) {\n",
        "                    throw new Error('WebGPU not supported in this browser');\n",
        "                }\n",
        "\n",
        "                updateStatus('\ud83d\udce6 Initializing WebLLM engine...');\n",
        "                engine = await webllm.CreateMLCEngine(\n",
        "                    modelConfig.model_id,\n",
        "                    { \n",
        "                        initProgressCallback: (progress) => {\n",
        "                            updateStatus(`\u23f3 Loading: ${progress.text}`);\n",
        "                        },\n",
        "                        appConfig: {\n",
        "                            model_list: [modelConfig]\n",
        "                        }\n",
        "                    }\n",
        "                );\n",
        "\n",
        "                updateStatus('\u2705 Model loaded! Ready to chat.');\n",
        "                userInput.disabled = false;\n",
        "                sendBtn.disabled = false;\n",
        "                userInput.focus();\n",
        "                \n",
        "                addMessage('system', 'WebLLM initialized with Vicuna-7B model. Start chatting!');\n",
        "            } catch (error) {\n",
        "                updateStatus(`\u274c Error: ${error.message}`, true);\n",
        "                console.error('Initialization error:', error);\n",
        "            }\n",
        "        }\n",
        "\n",
        "        window.sendMessage = async function() {\n",
        "            const message = userInput.value.trim();\n",
        "            if (!message || !engine) return;\n",
        "\n",
        "            addMessage('user', message);\n",
        "            userInput.value = '';\n",
        "            userInput.disabled = true;\n",
        "            sendBtn.disabled = true;\n",
        "            updateStatus('\ud83e\udd16 Generating response...');\n",
        "\n",
        "            try {\n",
        "                const messages = [{ role: 'user', content: message }];\n",
        "                let response = '';\n",
        "\n",
        "                const chunks = await engine.chat.completions.create({\n",
        "                    messages: messages,\n",
        "                    temperature: 0.7,\n",
        "                    max_tokens: 256,\n",
        "                    stream: true\n",
        "                });\n",
        "\n",
        "                const assistantDiv = document.createElement('div');\n",
        "                assistantDiv.className = 'message assistant-message';\n",
        "                chatBox.appendChild(assistantDiv);\n",
        "\n",
        "                for await (const chunk of chunks) {\n",
        "                    const delta = chunk.choices[0]?.delta?.content;\n",
        "                    if (delta) {\n",
        "                        response += delta;\n",
        "                        assistantDiv.textContent = response;\n",
        "                        chatBox.scrollTop = chatBox.scrollHeight;\n",
        "                    }\n",
        "                }\n",
        "\n",
        "                updateStatus('\u2705 Model loaded! Ready to chat.');\n",
        "            } catch (error) {\n",
        "                updateStatus(`\u274c Error: ${error.message}`, true);\n",
        "                addMessage('system', `Error: ${error.message}`);\n",
        "            } finally {\n",
        "                userInput.disabled = false;\n",
        "                sendBtn.disabled = false;\n",
        "                userInput.focus();\n",
        "            }\n",
        "        };\n",
        "\n",
        "        // Handle Enter key\n",
        "        userInput.addEventListener('keypress', (e) => {\n",
        "            if (e.key === 'Enter' && !sendBtn.disabled) {\n",
        "                sendMessage();\n",
        "            }\n",
        "        });\n",
        "\n",
        "        // Initialize on load\n",
        "        initEngine();\n",
        "    </script>\n",
        "</body>\n",
        "</html>\n",
        "'''\n",
        "\n",
        "# Save HTML file\n",
        "html_file = '/content/webllm_demo.html'\n",
        "with open(html_file, 'w') as f:\n",
        "    f.write(html_content)\n",
        "\n",
        "print(f\"\u2705 WebLLM demo HTML created: {html_file}\\n\")\n",
        "print(\"\ud83d\udcdd Note: In Colab, this demo requires:\")\n",
        "print(\"   1. WebGPU support (Chrome 113+ with flags enabled)\")\n",
        "print(\"   2. Proper CORS headers (use with local server or colab.research.google.com)\")\n",
        "print(\"\\n\ud83d\ude80 To test locally, you can:\")\n",
        "print(\"   1. Download the HTML file\")\n",
        "print(\"   2. Open it in a WebGPU-enabled browser\")\n",
        "print(\"   3. Or serve it with: python -m http.server 8000\\n\")\n",
        "\n",
        "# Display the HTML (note: full WebGPU may not work in Colab iframe)\n",
        "print(\"\ud83d\udcfa Displaying demo (WebGPU may be limited in Colab):\")\n",
        "display(HTML(html_content))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joksters_test",
        "cellView": "form"
      },
      "source": [
        "#@title \ud83c\udfad Simplified Joksters Routine Test\n",
        "import os\n",
        "import json\n",
        "from IPython.display import HTML, display, Javascript\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "MODEL_DIR = \"/content/ford442-vicuna-7b-q4f32\"\n",
        "\n",
        "print(\"\ud83c\udfad Setting up Joksters app test...\\n\")\n",
        "\n",
        "# Clone the joksters repository if not present\n",
        "REPO_DIR = \"/content/the_jokesters\"\n",
        "if not os.path.exists(REPO_DIR):\n",
        "    print(\"\ud83d\udce5 Cloning the_jokesters repository...\")\n",
        "    !git clone https://github.com/ford442/the_jokesters.git {REPO_DIR}\n",
        "else:\n",
        "    print(\"\u2705 Repository already cloned.\")\n",
        "\n",
        "# Install Node.js dependencies\n",
        "print(\"\\n\ud83d\udce6 Installing Node.js dependencies...\")\n",
        "os.chdir(REPO_DIR)\n",
        "!npm install\n",
        "\n",
        "# Create a test HTML that loads the Joksters app with the HF model\n",
        "print(\"\\n\ud83d\udd27 Creating test configuration...\")\n",
        "\n",
        "test_html = '''\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "    <title>Joksters Test - Vicuna 7B</title>\n",
        "    <style>\n",
        "        body {\n",
        "            margin: 0;\n",
        "            padding: 20px;\n",
        "            font-family: Arial, sans-serif;\n",
        "            background: #1a1a1a;\n",
        "            color: white;\n",
        "        }\n",
        "        #container {\n",
        "            max-width: 1200px;\n",
        "            margin: 0 auto;\n",
        "        }\n",
        "        h1 { color: #4fc3f7; }\n",
        "        #status {\n",
        "            background: #263238;\n",
        "            padding: 15px;\n",
        "            border-radius: 8px;\n",
        "            margin: 20px 0;\n",
        "            border-left: 4px solid #4fc3f7;\n",
        "        }\n",
        "        #chat-container {\n",
        "            background: #263238;\n",
        "            padding: 20px;\n",
        "            border-radius: 8px;\n",
        "            margin: 20px 0;\n",
        "            min-height: 400px;\n",
        "        }\n",
        "        .message {\n",
        "            margin: 10px 0;\n",
        "            padding: 12px;\n",
        "            border-radius: 6px;\n",
        "            line-height: 1.5;\n",
        "        }\n",
        "        .agent-message {\n",
        "            background: #37474f;\n",
        "            border-left: 3px solid #4fc3f7;\n",
        "        }\n",
        "        .user-message {\n",
        "            background: #1e3a5f;\n",
        "            border-left: 3px solid #64b5f6;\n",
        "        }\n",
        "        .agent-name {\n",
        "            font-weight: bold;\n",
        "            color: #4fc3f7;\n",
        "            margin-bottom: 5px;\n",
        "        }\n",
        "        button {\n",
        "            background: #4fc3f7;\n",
        "            color: #1a1a1a;\n",
        "            border: none;\n",
        "            padding: 12px 24px;\n",
        "            border-radius: 6px;\n",
        "            cursor: pointer;\n",
        "            font-size: 16px;\n",
        "            font-weight: bold;\n",
        "            margin: 10px 5px;\n",
        "        }\n",
        "        button:hover { background: #81d4fa; }\n",
        "        button:disabled {\n",
        "            background: #555;\n",
        "            cursor: not-allowed;\n",
        "        }\n",
        "        input {\n",
        "            width: calc(100% - 24px);\n",
        "            padding: 12px;\n",
        "            background: #37474f;\n",
        "            border: 1px solid #4fc3f7;\n",
        "            border-radius: 6px;\n",
        "            color: white;\n",
        "            font-size: 14px;\n",
        "            margin: 10px 0;\n",
        "        }\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <div id=\"container\">\n",
        "        <h1>\ud83c\udfad The Joksters - Test Mode</h1>\n",
        "        <p>Testing with Vicuna-7B model from HuggingFace (ford442/vicuna-7b-q4f32-webllm)</p>\n",
        "        \n",
        "        <div id=\"status\">\u23f3 Initializing WebGPU and loading model...</div>\n",
        "        \n",
        "        <div id=\"controls\" style=\"display:none;\">\n",
        "            <input type=\"text\" id=\"user-input\" placeholder=\"Type your message to the agents...\" />\n",
        "            <button onclick=\"sendMessage()\">Send Message</button>\n",
        "            <button onclick=\"startImprov()\">Start Improv Scene</button>\n",
        "        </div>\n",
        "        \n",
        "        <div id=\"chat-container\"></div>\n",
        "    </div>\n",
        "\n",
        "    <script type=\"module\">\n",
        "        import * as webllm from \"https://esm.run/@mlc-ai/web-llm\";\n",
        "\n",
        "        let engine = null;\n",
        "        let currentAgent = 0;\n",
        "        const agents = [\n",
        "            { name: 'The Comedian', color: '#ff5252', temp: 0.9 },\n",
        "            { name: 'The Philosopher', color: '#26c6da', temp: 0.7 },\n",
        "            { name: 'The Scientist', color: '#42a5f5', temp: 0.3 }\n",
        "        ];\n",
        "\n",
        "        const statusDiv = document.getElementById('status');\n",
        "        const chatContainer = document.getElementById('chat-container');\n",
        "        const controlsDiv = document.getElementById('controls');\n",
        "        const userInput = document.getElementById('user-input');\n",
        "\n",
        "        function updateStatus(message) {\n",
        "            statusDiv.textContent = message;\n",
        "        }\n",
        "\n",
        "        function addMessage(agentName, content, color) {\n",
        "            const div = document.createElement('div');\n",
        "            div.className = 'message agent-message';\n",
        "            div.innerHTML = `\n",
        "                <div class=\"agent-name\" style=\"color: ${color}\">${agentName}</div>\n",
        "                <div>${content}</div>\n",
        "            `;\n",
        "            chatContainer.appendChild(div);\n",
        "            chatContainer.scrollTop = chatContainer.scrollHeight;\n",
        "        }\n",
        "\n",
        "        async function initEngine() {\n",
        "            try {\n",
        "                updateStatus('\ud83d\udd27 Checking WebGPU support...');\n",
        "                \n",
        "                if (!navigator.gpu) {\n",
        "                    throw new Error('WebGPU not supported. Please use Chrome 113+ with WebGPU enabled.');\n",
        "                }\n",
        "\n",
        "                updateStatus('\ud83d\udce6 Loading Vicuna-7B model from HuggingFace...');\n",
        "                \n",
        "                const modelConfig = {\n",
        "                    model_id: \"ford442/vicuna-7b-q4f32-webllm\",\n",
        "                    model: \"https://huggingface.co/ford442/vicuna-7b-q4f32-webllm/resolve/main/\",\n",
        "                    model_lib: \"https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Llama-2-7b-chat-hf-q4f32_1-ctx4k_cs1k-webgpu.wasm\",\n",
        "                    vram_required_MB: 4096,\n",
        "                    low_resource_required: false,\n",
        "                };\n",
        "\n",
        "                engine = await webllm.CreateMLCEngine(\n",
        "                    modelConfig.model_id,\n",
        "                    { \n",
        "                        initProgressCallback: (progress) => {\n",
        "                            updateStatus(`\u23f3 Loading: ${progress.text}`);\n",
        "                        },\n",
        "                        appConfig: {\n",
        "                            model_list: [modelConfig]\n",
        "                        }\n",
        "                    }\n",
        "                );\n",
        "\n",
        "                updateStatus('\u2705 Model loaded! The Joksters are ready to perform.');\n",
        "                controlsDiv.style.display = 'block';\n",
        "                \n",
        "                addMessage('System', 'Welcome! Three AI agents are ready: The Comedian, The Philosopher, and The Scientist. Send a message or start an improv scene!', '#4fc3f7');\n",
        "            } catch (error) {\n",
        "                updateStatus(`\u274c Error: ${error.message}`);\n",
        "                console.error('Initialization error:', error);\n",
        "            }\n",
        "        }\n",
        "\n",
        "        window.sendMessage = async function() {\n",
        "            const message = userInput.value.trim();\n",
        "            if (!message || !engine) return;\n",
        "\n",
        "            const agent = agents[currentAgent];\n",
        "            userInput.value = '';\n",
        "            \n",
        "            addMessage('You', message, '#64b5f6');\n",
        "            updateStatus(`\ud83e\udd16 ${agent.name} is responding...`);\n",
        "\n",
        "            try {\n",
        "                const systemPrompt = `You are ${agent.name}. ${agent.name === 'The Comedian' ? \n",
        "                    'You are witty and humorous.' : agent.name === 'The Philosopher' ?\n",
        "                    'You are thoughtful and profound.' : 'You are logical and precise.'} Keep responses brief.`;\n",
        "                \n",
        "                const messages = [\n",
        "                    { role: 'system', content: systemPrompt },\n",
        "                    { role: 'user', content: message }\n",
        "                ];\n",
        "\n",
        "                let response = '';\n",
        "                const chunks = await engine.chat.completions.create({\n",
        "                    messages: messages,\n",
        "                    temperature: agent.temp,\n",
        "                    max_tokens: 150,\n",
        "                    stream: true\n",
        "                });\n",
        "\n",
        "                for await (const chunk of chunks) {\n",
        "                    const delta = chunk.choices[0]?.delta?.content;\n",
        "                    if (delta) response += delta;\n",
        "                }\n",
        "\n",
        "                addMessage(agent.name, response, agent.color);\n",
        "                currentAgent = (currentAgent + 1) % agents.length;\n",
        "                updateStatus('\u2705 Ready for next message');\n",
        "            } catch (error) {\n",
        "                updateStatus(`\u274c Error: ${error.message}`);\n",
        "                addMessage('System', `Error: ${error.message}`, '#ff5252');\n",
        "            }\n",
        "        };\n",
        "\n",
        "        window.startImprov = async function() {\n",
        "            if (!engine) return;\n",
        "            \n",
        "            updateStatus('\ud83c\udfad Starting improv scene...');\n",
        "            addMessage('System', 'Starting an improv comedy scene: \"At the Coffee Shop\"', '#4fc3f7');\n",
        "            \n",
        "            const scene = \"Three friends meet at a coffee shop to discuss their latest adventures with AI.\";\n",
        "            \n",
        "            for (let i = 0; i < 3; i++) {\n",
        "                const agent = agents[i];\n",
        "                updateStatus(`\ud83c\udfad ${agent.name} is improvising...`);\n",
        "                \n",
        "                try {\n",
        "                    const messages = [\n",
        "                        { role: 'system', content: `You are ${agent.name} in an improv scene: \"${scene}\". Stay in character and contribute to the scene with a brief response.` },\n",
        "                        { role: 'user', content: 'Continue the scene.' }\n",
        "                    ];\n",
        "\n",
        "                    let response = '';\n",
        "                    const chunks = await engine.chat.completions.create({\n",
        "                        messages: messages,\n",
        "                        temperature: agent.temp,\n",
        "                        max_tokens: 100,\n",
        "                        stream: true\n",
        "                    });\n",
        "\n",
        "                    for await (const chunk of chunks) {\n",
        "                        const delta = chunk.choices[0]?.delta?.content;\n",
        "                        if (delta) response += delta;\n",
        "                    }\n",
        "\n",
        "                    addMessage(agent.name, response, agent.color);\n",
        "                } catch (error) {\n",
        "                    addMessage('System', `Error with ${agent.name}: ${error.message}`, '#ff5252');\n",
        "                }\n",
        "            }\n",
        "            \n",
        "            updateStatus('\u2705 Improv scene complete!');\n",
        "        };\n",
        "\n",
        "        userInput.addEventListener('keypress', (e) => {\n",
        "            if (e.key === 'Enter') sendMessage();\n",
        "        });\n",
        "\n",
        "        initEngine();\n",
        "    </script>\n",
        "</body>\n",
        "</html>\n",
        "'''\n",
        "\n",
        "# Save the test HTML\n",
        "test_file = '/content/joksters_test.html'\n",
        "with open(test_file, 'w') as f:\n",
        "    f.write(test_html)\n",
        "\n",
        "print(f\"\\n\u2705 Joksters test HTML created: {test_file}\")\n",
        "print(\"\\n\ud83d\udcdd This simplified test demonstrates:\")\n",
        "print(\"   \u2022 Loading the Vicuna-7B model from HuggingFace\")\n",
        "print(\"   \u2022 Three AI agents with distinct personalities\")\n",
        "print(\"   \u2022 Interactive chat with rotating agents\")\n",
        "print(\"   \u2022 Improv scene generation\")\n",
        "print(\"   \u2022 WebGPU-powered inference in the browser\")\n",
        "print(\"\\n\ud83d\ude80 To test:\")\n",
        "print(\"   1. Download the HTML file\")\n",
        "print(\"   2. Open in Chrome 113+ with WebGPU enabled\")\n",
        "print(\"   3. Wait for model to load (~4GB download)\")\n",
        "print(\"   4. Interact with the agents!\\n\")\n",
        "\n",
        "# Display in Colab\n",
        "print(\"\ud83d\udcfa Displaying test interface:\")\n",
        "display(HTML(test_html))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "test_notes"
      },
      "source": [
        "## \ud83d\udccb Testing Notes\\n\",\n",
        "\\n\",\n",
        "### WebGPU Requirements\\n\",\n",
        "\\n\",\n",
        "To run WebLLM with WebGPU in Colab:\\n\",\n",
        "\\n\",\n",
        "1. **Browser Support**: Chrome 113+ or Edge 113+\\n\",\n",
        "2. **Enable WebGPU**: `chrome://flags/#enable-unsafe-webgpu`\\n\",\n",
        "3. **CORS Headers**: Required for cross-origin isolation\\n\",\n",
        "4. **GPU Access**: Browser needs access to GPU\\n\",\n",
        "\\n\",\n",
        "### Model Information\\n\",\n",
        "\\n\",\n",
        "- **Model**: ford442/vicuna-7b-q4f32-webllm\\n\",\n",
        "- **Size**: ~4GB download\\n\",\n",
        "- **Quantization**: q4f32_1 (4-bit weights, 32-bit activations)\\n\",\n",
        "- **Context**: 4096 tokens\\n\",\n",
        "- **VRAM**: ~4GB required\\n\",\n",
        "\\n\",\n",
        "### Limitations in Colab\\n\",\n",
        "\\n\",\n",
        "- WebGPU may not work in Colab's iframe environment\\n\",\n",
        "- Full testing requires downloading HTML and running locally\\n\",\n",
        "- CLI test (Cell 2) works fully in Colab with GPU runtime\\n\",\n",
        "- Web demos (Cells 3-4) are best tested outside Colab\\n\",\n",
        "\\n\",\n",
        "### Alternative Testing\\n\",\n",
        "\\n\",\n",
        "For full WebGPU testing:\\n\",\n",
        "1. Download the generated HTML files\\n\",\n",
        "2. Serve locally: `python -m http.server 8000`\\n\",\n",
        "3. Open in WebGPU-enabled browser\\n\",\n",
        "4. Ensure proper CORS headers are set\"\n"
      ]
    }
  ]
}